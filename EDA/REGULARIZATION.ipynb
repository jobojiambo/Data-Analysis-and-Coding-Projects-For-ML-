{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab67b5f",
   "metadata": {},
   "source": [
    "# REGULARIZATION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "512c782b",
   "metadata": {},
   "source": [
    "Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the model's objective function, encouraging it to generalize well to unseen data. It is particularly useful when dealing with high-dimensional datasets or when the number of features is much larger than the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3127b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e649335",
   "metadata": {},
   "source": [
    "Regularization is a technique used in machine learning to prevent overfitting and improve the generalization of the model by adding a penalty term to the loss function of the model to discourage it from fitting the noise in the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0769e",
   "metadata": {},
   "source": [
    "### Creating the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543250bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.737316</td>\n",
       "      <td>0.379297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.418457</td>\n",
       "      <td>-0.536133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.628665</td>\n",
       "      <td>0.348569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.312796</td>\n",
       "      <td>0.127247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.066724</td>\n",
       "      <td>0.469227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.051047</td>\n",
       "      <td>1.439409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.371654</td>\n",
       "      <td>0.109466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.853134</td>\n",
       "      <td>0.645480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.891279</td>\n",
       "      <td>2.152187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.271971</td>\n",
       "      <td>-0.007723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x         y\n",
       "0  -1.737316  0.379297\n",
       "1  -0.418457 -0.536133\n",
       "2  -0.628665  0.348569\n",
       "3  -0.312796  0.127247\n",
       "4  -1.066724  0.469227\n",
       "..       ...       ...\n",
       "95 -0.051047  1.439409\n",
       "96  0.371654  0.109466\n",
       "97 -0.853134  0.645480\n",
       "98 -0.891279  2.152187\n",
       "99  1.271971 -0.007723\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'x': np.random.normal(size=100), 'y': np.random.normal(size=100)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5de3d5",
   "metadata": {},
   "source": [
    "### Adding polynomial features to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a633c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 50):\n",
    "    colname = 'x_%d' % i\n",
    "    df[colname] = df['x'] ** i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe9d1cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate dependent and independent variables\n",
    "independent_variables = list(df.columns)\n",
    "independent_variables.remove('y')\n",
    "X = df[independent_variables]\n",
    "y = df.y\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606b39e",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc160df9",
   "metadata": {},
   "source": [
    "Ridge regression, also known as L2 regularization, is a type of linear regression that adds a penalty to the sum of squares of the coefficients. This penalty helps to shrink or reduce the coefficients of the least important variables, effectively performing feature selection and reducing overfitting. \n",
    "Ridge regression is particularly useful when dealing with multicollinearity in the data, where the independent variables are highly correlated with each other. By reducing the impact of these variables, ridge regression can improve the stability and reliability of the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a56fad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Ridge Regression ------\n",
      "Train MAE:  0.75984725666159\n",
      "Train RMSE:  0.9545954972449561\n",
      "Test MAE:  0.6569376480790978\n",
      "Test RMSE:  0.8954746597248873\n",
      "Ridge Coef:  [-5.04758601e-12  9.81769215e-13  3.54362390e-12 -1.44500939e-12\n",
      " -2.57920958e-12  1.51091281e-13  1.18566763e-12 -9.82583584e-13\n",
      " -8.57454964e-13 -2.85920084e-12 -9.79057423e-13  6.67941872e-13\n",
      "  1.92704203e-12  5.10764248e-12 -3.57434439e-13  2.40522907e-13\n",
      " -1.30133086e-12  1.04455286e-12 -8.80808989e-13  3.33065406e-12\n",
      " -3.49902359e-12  9.22374616e-13  6.61708139e-13  8.05729209e-13\n",
      " -2.99949816e-12 -1.76234528e-12  1.37683290e-12  2.83212485e-12\n",
      " -1.94522442e-12 -2.24364070e-12 -2.77604377e-13  7.55900420e-13\n",
      "  3.87773692e-12  1.60343286e-12 -2.97581619e-13  3.78589802e-12\n",
      "  4.11869418e-12  3.06073048e-13  1.61822741e-12 -1.81886756e-12\n",
      " -1.59313882e-12 -4.13632417e-13 -1.31600117e-12  1.29684735e-12\n",
      " -2.14786531e-13  2.03031104e-12 -1.20140931e-12 -1.03627763e-13\n",
      "  1.05278629e-13]\n"
     ]
    }
   ],
   "source": [
    "lr = linear_model.Ridge(alpha=0.001)\n",
    "lr.fit(X_train, y_train)\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"------ Ridge Regression ------\")\n",
    "print(\"Train MAE: \", metrics.mean_absolute_error(y_train, y_train_pred))\n",
    "print(\"Train RMSE: \", np.sqrt(metrics.mean_squared_error(y_train, y_train_pred)))\n",
    "print(\"Test MAE: \", metrics.mean_absolute_error(y_test, y_test_pred))\n",
    "print(\"Test RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
    "print(\"Ridge Coef: \", lr.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca96873",
   "metadata": {},
   "source": [
    "### LASSO Regression "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a2708cd",
   "metadata": {},
   "source": [
    "Lasso regression, also known as L1 regularization, is a type of linear regression that adds a penalty to the sum of absolute values of the coefficients. This penalty helps to shrink or eliminate the coefficients of the least important variables, effectively performing feature selection and reducing overfitting. \n",
    "Contrary to Ridge regression, Lasso regression is particularly useful when dealing with high-dimensional datasets with many variables, as it can effectively identify the most important variables and discard the irrelevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bbff378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- LASSO Regression -----\n",
      "Train MAE:  0.6715472938285949\n",
      "Train RMSE:  0.8436385363167183\n",
      "Test MAE:  5.3816917528852635\n",
      "Test RMSE:  20.340182489461387\n",
      "LASSO Coef:  [-4.74493020e-01 -1.21720596e+00  7.56816260e-01  6.77167199e-01\n",
      " -2.64399145e-01 -5.69484076e-02  7.14210932e-03 -1.81169430e-02\n",
      "  4.94786871e-03 -5.77700851e-04  8.25226104e-04  1.96475786e-04\n",
      "  9.09531669e-05  4.93517222e-05  4.91924743e-06  7.30448456e-06\n",
      " -8.90095430e-07  7.60780492e-07 -3.99279618e-07  3.47513663e-08\n",
      " -1.00467135e-07 -9.01412891e-09 -2.10384260e-08 -3.63574944e-09\n",
      " -4.01554378e-09 -8.92472202e-10 -7.24920748e-10 -1.85158412e-10\n",
      " -1.26166808e-10 -3.52481815e-11 -2.14089359e-11 -6.37116799e-12\n",
      " -3.56754680e-12 -1.11297666e-12 -5.86659701e-13 -1.89871032e-13\n",
      " -9.55284899e-14 -3.18429480e-14 -1.54412984e-14 -5.27329335e-15\n",
      " -2.48215708e-15 -8.64996532e-16 -3.97336693e-16 -1.40857867e-16\n",
      " -6.34038939e-17 -2.28084507e-17 -1.00934019e-17 -3.67698977e-18\n",
      " -1.60390942e-18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.875e+01, tolerance: 6.407e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "lr = linear_model.Lasso(alpha=0.001) \n",
    "lr.fit(X_train, y_train) \n",
    "y_train_pred = lr.predict(X_train) \n",
    "y_test_pred = lr.predict(X_test)\n",
    "print(\"----- LASSO Regression -----\") \n",
    "print(\"Train MAE: \", metrics.mean_absolute_error(y_train, y_train_pred) )\n",
    "print(\"Train RMSE: \", np.sqrt(metrics.mean_squared_error(y_train, y_train_pred)))\n",
    "print(\"Test MAE: \", metrics.mean_absolute_error(y_test, y_test_pred))\n",
    "print(\"Test RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
    "print(\"LASSO Coef: \", lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bff11f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
